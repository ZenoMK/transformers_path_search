{
    "vocab_size": 50000,
    "context_length": 50,
    "emb_dim": 120,
    "n_heads": 10,
    "n_layers": 1,
    "drop_rate": 0.1,
    "qkv_bias": true,
    "batch_size": 5,
    "learning_rate": 0.0004,
    "weight_decay": 0.1,
    "attention_type": "MHGQA",
    "kv_heads": 2
}